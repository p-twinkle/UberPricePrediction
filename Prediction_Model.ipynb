{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "LsfPFlcND83G",
    "outputId": "25df5e95-41df-4ddc-8d7d-8184b4a41216"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(file, \"rt\"):\n",
      "“cannot open file 'uber.csv': No such file or directory”\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(file, \"rt\"): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in file(file, \"rt\"): cannot open the connection\nTraceback:\n",
      "1. read.csv(\"uber.csv\")",
      "2. read.table(file = file, header = header, sep = sep, quote = quote, \n .     dec = dec, fill = fill, comment.char = comment.char, ...)",
      "3. file(file, \"rt\")"
     ]
    }
   ],
   "source": [
    "# ---------------------------Google API Call for Map distance----------------------------\n",
    "\n",
    "# Read raw uber data file\n",
    "set.seed(1)\n",
    "raw_data <- read.csv(\"uber.csv\")\n",
    "dim(raw_data)\n",
    "\n",
    "# User Input\n",
    "# API_key <- 'Enter_your_api_key'\n",
    "\n",
    "# function to get map distance from Google Maps API\n",
    "get_map_distance <- function(pickup, dropoff, API_key)\n",
    "  {\n",
    "  # Construct the URL\n",
    "  url <- paste0(\"https://maps.googleapis.com/maps/api/distancematrix/json?origins=\",\n",
    "                pickup, \"&destinations=\", dropoff, \"&key=\", API_key)\n",
    "\n",
    "  # Make the request\n",
    "  response <- httr::GET(url)\n",
    "\n",
    "  # Parse the response\n",
    "  response_content <- httr::content(response, as = \"text\")\n",
    "  response_json <- jsonlite::fromJSON(response_content)\n",
    "\n",
    "  # Check the status and extract the distance\n",
    "  if (response_json$status == \"OK\")\n",
    "    {\n",
    "    # Get the distance in meters\n",
    "    distance_meters <- response_json$rows$elements[[1]]$distance$value\n",
    "    # Convert the distance to kilometers\n",
    "    distance_miles <- distance_meters / 1609.344\n",
    "    print(paste(\"Distance:\", distance_miles, \"miles\"))\n",
    "    }\n",
    "  else\n",
    "    {\n",
    "    print(\"Error:\", response_json$error_message)\n",
    "    distance_miles <- NULL\n",
    "    }\n",
    "  return(distance_miles)\n",
    " }\n",
    "\n",
    "# new_data <- head(raw_data)\n",
    "new_data <- raw_data %>%\n",
    "  mutate(pickup=paste(pickup_latitude, pickup_longitude, sep=','),\n",
    "         dropoff=paste(dropoff_latitude, dropoff_longitude, sep=',')\n",
    "         )\n",
    "\n",
    "# final data\n",
    "df <- new_data[new_data$pickup != new_data$dropoff, ]\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set.seed(123)\n",
    "\n",
    "# Function to split the data frame into batches of 20000 rows\n",
    "split_into_batches <- function(df, batch_size) {\n",
    "  split(df, ceiling(seq_along(rownames(df)) / batch_size))\n",
    "}\n",
    "\n",
    "# Split the data frame into batches of 20000 rows\n",
    "batch_size <- 20000\n",
    "batches <- split_into_batches(df, batch_size)\n",
    "\n",
    "# Print the number of batches and the first few rows of each batch\n",
    "length(batches)\n",
    "\n",
    "# Define the function to process each batch and get the map distance using API call\n",
    "process_batch <- function(batch) {\n",
    "  batch$distance_miles <- apply(batch, 1, function(row){\n",
    "    get_map_distance(\n",
    "      pickup = row[\"pickup\"]\n",
    "      ,dropoff = row[\"dropoff\"]\n",
    "      ,API_key = API_key)\n",
    "  })\n",
    "  return(batch)\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store the processed batches\n",
    "processed_batches <- list()\n",
    "\n",
    "# Process each batch and store the results in processed_batches\n",
    "for (i in seq_along(batches)) {\n",
    "  processed_batches[[i]] <- process_batch(batches[[i]], API_key)\n",
    "  print(paste(\"Processed batch\", i, \"of\", length(batches)))\n",
    "}\n",
    "\n",
    "# Combine all processed batches to form the final data\n",
    "final_df <- do.call(rbind, processed_batches)\n",
    "# write the csv to use later\n",
    "write.csv(final_df, \"uber_google_api_distance.csv\")\n",
    "\n",
    "# ---------------------------------------- FINAL CODE --------------------------------------\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Pre-processing\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "\n",
    "data <- read.csv(\"uber_google_api_distance.csv\")\n",
    "data <- distinct(data)\n",
    "\n",
    "# The first column is key and the 2nd column is just time, but the column names are incorrect\n",
    "# we don't need only time, as we have pickup_datetime as a separate feature\n",
    "# drop the time column which is named incrrectly and name the key column correctly\n",
    "\n",
    "data <- data %>%\n",
    "  select(-key) %>%\n",
    "  drop_na()\n",
    "\n",
    "colnames(data)[colnames(data) == \"X\"] = \"key\"\n",
    "\n",
    "# ----------- Append UberXL Flag ------------------------------------------------\n",
    "\n",
    "data <- data %>%\n",
    "  mutate(uberxl_flag = factor(ifelse((passenger_count>4), TRUE, FALSE)))\n",
    "\n",
    "#filter data for non zero lat-long\n",
    "data_long_lat_correction <- data %>%\n",
    "  filter(pickup_longitude != 0 &\n",
    "          pickup_latitude != 0 &\n",
    "          dropoff_longitude != 0 &\n",
    "          dropoff_latitude != 0,\n",
    "         fare_amount > 1,\n",
    "         pickup_longitude >= -180 & pickup_longitude <= 180 &\n",
    "        dropoff_longitude >= -180 & dropoff_longitude <= 180 &\n",
    "        pickup_latitude >= -90 & pickup_latitude <= 90 &\n",
    "        dropoff_latitude >= -90 & dropoff_latitude <= 90\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    distance_miles = as.numeric(distance_miles)\n",
    "  )\n",
    "\n",
    "# split the pickup_datetime column and create a dateframe of 2 columns : date and time\n",
    "\n",
    "split_time <- strsplit(data_long_lat_correction$pickup_datetime, \" \")\n",
    "\n",
    "split_df <- do.call(rbind, split_time)\n",
    "\n",
    "split_df <- as.data.frame(split_df, stringsAsFactors = FALSE)\n",
    "colnames(split_df) <- c(\"date\", \"time\", \"UTC\")\n",
    "\n",
    "# drop the UTC column\n",
    "split_df <- subset(split_df, select = -UTC)\n",
    "\n",
    "data_date_time <- cbind(data_long_lat_correction, split_df)\n",
    "\n",
    "data_uber <- data_date_time %>%\n",
    "  mutate(\n",
    "    p_date = ymd(date),\n",
    "    p_time = chron(times=time),\n",
    "    day = weekdays(p_date),\n",
    "    year = year(p_date),\n",
    "    month = month(p_date),\n",
    "    weekday = factor(ifelse((day==\"Sunday\" | day==\"Saturday\"), \"No\", \"Yes\")),\n",
    "    seasons = factor(case_when(month %in% c(12, 1, 2) ~ \"Winter\",\n",
    "                      month %in% c(3, 4, 5) ~ \"Spring\",\n",
    "                      month %in% c(6, 7, 8) ~ \"Summer\",\n",
    "                      month %in% c(9,10,11) ~ \"Fall\")),\n",
    "    pickuptimeOfDay = factor(case_when(between(p_time, chron(times=c('06:00:00')),\n",
    "                                        chron(times=c('11:59:59'))) ~ \"Morning\",\n",
    "                                between(p_time, chron(times=c('12:00:00')),\n",
    "                                        chron(times=c('16:59:59'))) ~ \"Afternoon\",\n",
    "                                between(p_time, chron(times=c('17:00:00')),\n",
    "                                        chron(times=c('20:59:59'))) ~ \"Evening\",\n",
    "                                between(p_time, chron(times=c('21:00:00')),\n",
    "                                        chron(times=c('23:59:59'))) ~ \"Night\",\n",
    "                                between(p_time, chron(times=c('00:00:00')),\n",
    "                                        chron(times=c('05:59:59'))) ~ \"Night\")),\n",
    "    passenger_count = cut(passenger_count, breaks=c(0, 2, 4, 6))\n",
    "  ) %>%\n",
    "  select(-date, -time, -month,\n",
    "         -pickup_datetime,\n",
    "         -pickup, -dropoff,\n",
    "          -p_time, -zipcode,\n",
    "         -key, -day, -seasons,\n",
    "         -pickup_longitude, -pickup_latitude,\n",
    "         -dropoff_longitude, -dropoff_latitude)  %>%\n",
    "  drop_na()\n",
    "\n",
    "\n",
    "# ----------- Append Holiday Flag ---------------------------------------\n",
    "\n",
    "library(timeDate)\n",
    "\n",
    "# Get holidays for each year and combine into a single vector\n",
    "years <- min(data_uber$year):max(data_uber$year)\n",
    "us_holidays_list <- lapply(years, function(year) {\n",
    "  as.Date(holidayNYSE(year))\n",
    "})\n",
    "\n",
    "# Combine them to create a data frame of holiday list and set flag to TRUE\n",
    "all_holidays <- do.call(c, us_holidays_list)\n",
    "us_holidays_df <- data.frame(p_date = all_holidays)\n",
    "us_holidays_df$holiday_flag <- TRUE\n",
    "\n",
    "# Append holidays to original data\n",
    "data_uber <- data_uber %>%\n",
    "  left_join(us_holidays_df, by = \"p_date\") %>%\n",
    "  mutate(holiday_flag = factor(ifelse(!is.na(holiday_flag), TRUE, FALSE)) )%>%\n",
    "  select(-p_date) %>%\n",
    "  mutate(year=factor(year))\n",
    "\n",
    "head(data_uber)\n",
    "summary(data_uber)\n",
    "dim(data_uber)\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Linear Models\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "linear_model <- lm(fare_amount~., data = train_data)\n",
    "summary(linear_model)\n",
    "\n",
    "yhat_lm <- predict(linear_model, test_data)\n",
    "\n",
    "y_test <- test_data$fare_amount\n",
    "\n",
    "test_mse_lm <- sqrt(mean((yhat_lm - y_test)^2))\n",
    "test_mse_lm\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Stepwise regression\n",
    "#--------------------------------------------------------------------\n",
    "stepwise <- stepAIC(linear_model, direction = \"both\")\n",
    "summary(stepwise)\n",
    "\n",
    "yhat_sw <- predict(stepwise, test_data)\n",
    "\n",
    "test_mse_sw <- sqrt(mean((yhat_sw - y_test)^2))\n",
    "test_mse_sw\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Boosting - XGBoost & GBM\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "library(caret)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set.seed(1)\n",
    "\n",
    "# Train-Test split\n",
    "# Create training index\n",
    "trainIndex <- createDataPartition(data_uber$fare_amount, p = 0.8, list = FALSE)\n",
    "\n",
    "# Create the training and testing sets\n",
    "trainData <- data_uber[trainIndex, ]\n",
    "testData <- data_uber[-trainIndex, ]\n",
    "\n",
    "# values that define how the train function must act\n",
    "fit_control <- trainControl(\n",
    "  method = \"cv\",\n",
    "  number = 10,\n",
    "  selectionFunction=\"oneSE\")\n",
    "\n",
    "#------------------------------ Boosting: GBM --------------------------------------------------\n",
    "\n",
    "library(gbm)\n",
    "\n",
    "# Boosting, optimizing over default grid for number of trees and depth\n",
    "gbmfit <- train(fare_amount ~ ., data = data_uber,\n",
    "                 method = \"gbm\",\n",
    "                 trControl = fit_control,\n",
    "                 verbose = FALSE)\n",
    "\n",
    "# Using a custom grid for tuning\n",
    "gbm_grid <-  expand.grid(interaction.depth = c(1, 2, 3, 5, 10), # depth of tree\n",
    "                         n.trees = c(150, 500, 1000), # number of trees\n",
    "                         shrinkage = c(0.01, 0.1, 0.2), # lambda or shrinkage\n",
    "                         n.minobsinnode = 10 # number of min obs. required for each node\n",
    ")\n",
    "gbmfit_2 <- train(fare_amount ~ ., data = data_uber, # . means all features\n",
    "                  method = \"gbm\", # what ML model to use\n",
    "                  trControl = fit_control,\n",
    "                  tuneGrid = gbm_grid,\n",
    "                  verbose = FALSE\n",
    ")\n",
    "print(gbmfit_2)\n",
    "# RMSE was used to select the optimal model using  the one SE rule.\n",
    "# The final values used for the model were n.trees = 150, interaction.depth = 1, shrinkage = 0.1\n",
    "# and n.minobsinnode = 10; RMSE: 4.436574\n",
    "\n",
    "print(gbmfit_2$results)\n",
    "\n",
    "# Determine the max RMSE that's within one SE of best\n",
    "best_ix = which.min(gbmfit_2$results$RMSE) # Gives index of the best i.e. min\n",
    "best = gbmfit_2$results[best_ix,]\n",
    "onese_max_RMSE = best$RMSE + best$RMSESD/sqrt(10)\n",
    "\n",
    "# These are the parameter values within one SD:\n",
    "onese_ixs = gbmfit_2$results$RMSE<onese_max_RMSE\n",
    "\n",
    "print(gbmfit_2$results[onese_ixs,]) # We have taken the least complex of all of these (1st row)\n",
    "\n",
    "# Visualization\n",
    "plot(gbmfit_2)\n",
    "ggplot(gbmfit_2)\n",
    "\n",
    "gbm_plot_df = gbmfit_2$results\n",
    "gbm_plot_df$n.trees = factor(gbm_plot_df$n.trees)\n",
    "\n",
    "kcv=10\n",
    "ggplot(aes(x=interaction.depth, y=RMSE, color=n.trees),\n",
    "       data=gbm_plot_df) +\n",
    "  facet_grid(~shrinkage, labeller = label_both) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  geom_segment(aes(x=interaction.depth,\n",
    "                   xend=interaction.depth,\n",
    "                   y=RMSE-RMSESD/sqrt(kcv),\n",
    "                   yend=RMSE+RMSESD/sqrt(kcv))) +\n",
    "  geom_hline(yintercept = onese_max_RMSE, linetype='dotted') +\n",
    "  xlab(\"Max Tree Depth\") +\n",
    "  ylab(\"RMSE (CV)\") +\n",
    "  scale_color_discrete(name = \"Num Boosting Iter\") +\n",
    "  theme(legend.position=\"bottom\")\n",
    "\n",
    "# On our validation set:\n",
    "gbm_yhat = predict(gbmfit_2, newdata=testData)\n",
    "\n",
    "# Validation RMSE\n",
    "sqrt(mean((testData$fare_amount - gbm_yhat)^2 )) # 4.374432\n",
    "\n",
    "# Comparing variable importance\n",
    "gbm_imp = varImp(gbmfit_2)\n",
    "combined_df = data.frame(variable=rownames(gbm_imp$importance),\n",
    "                         gbm = gbm_imp$importance$Overall)\n",
    "\n",
    "#------------------------------ Boosting: XGBoost --------------------------------------------------\n",
    "\n",
    "xgb_fit_control <- trainControl(\n",
    "  method = \"cv\",\n",
    "  number = 10,\n",
    "  selectionFunction=\"oneSE\"\n",
    "  )\n",
    "\n",
    "# Tuning grid\n",
    "xgb_grid <- expand.grid(nrounds = c(150, 500, 1000),  # Number of boosting rounds\n",
    "                        max_depth = c(4, 6, 8),  # Maximum depth of a tree\n",
    "                        eta = c(0.01, 0.1, 0.3),  # Learning rate\n",
    "                        gamma = 0,  # Minimum loss reduction\n",
    "                        colsample_bytree = 1,  # Subsample ratio of columns\n",
    "                        min_child_weight = 10,  # Minimum sum of instance weight\n",
    "                        subsample = c(0.8,1)  # Subsample ratio of the training instances\n",
    ")\n",
    "\n",
    "# Boosting, optimizing over default grid for number of trees and depth\n",
    "xgbfit <- train(fare_amount ~ ., data = data_uber,\n",
    "                method = \"xgbTree\",\n",
    "                trControl = xgb_fit_control,\n",
    "                tuneGrid = xgb_grid,\n",
    "                verbose = FALSE)\n",
    "\n",
    "print(xgbfit)\n",
    "# RMSE was used to select the optimal model using  the one SE rule.\n",
    "# The final values used for the model were nrounds = 50, max_depth = 4, eta = 0.1, gamma = 0,\n",
    "# colsample_bytree = 1, min_child_weight = 10 and subsample = 0.8.\n",
    "# RMSE -- 4.276331\n",
    "\n",
    "print(xgbfit$results)\n",
    "\n",
    "# On our validation set:\n",
    "xgb_yhat = predict(xgbfit, newdata=testData)\n",
    "\n",
    "# Validation RMSE\n",
    "sqrt(mean((testData$fare_amount - xgb_yhat)^2 )) # 3.920847-- with oneSE, 4.003607 with Best\n",
    "\n",
    "# Comparing variable importance\n",
    "xgb_imp <- varImp(xgbfit, scale = TRUE)\n",
    "plot(xgb_imp)\n",
    "imp_df = data.frame(variable=rownames(xgb_imp$importance),\n",
    "                         xgb = xgb_imp$importance$Overall)\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# BART \n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "# BART implementation (Sarthak)\n",
    "library(BART)\n",
    "set.seed(1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x <- data_uber %>%\n",
    "    select(-fare_amount)\n",
    "y <- data_uber$fare_amount\n",
    "trainIndex <- createDataPartition(y, p = 0.8, list = FALSE)\n",
    "\n",
    "x_train <- x[trainIndex, ]\n",
    "x_test <- x[-trainIndex, ]\n",
    "y_train <- y[trainIndex]\n",
    "y_test <- y[-trainIndex]\n",
    "\n",
    "#Model Training and Prediction\n",
    "bartfit <- wbart(x.train =  x_train, y.train = y_train, x.test = x_test)\n",
    "yhat_train <- bartfit$yhat.train.mean\n",
    "yhat_test <- bartfit$yhat.test.mean\n",
    "\n",
    "#Train and Validation RMSE\n",
    "bart_train_rmse <- sqrt(mean((yhat_train - y_train)^2))\n",
    "bart_test_rmse <- sqrt(mean((yhat_test - y_test)^2))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
